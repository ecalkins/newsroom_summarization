{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence to Sequence - Article Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T03:04:22.643999Z",
     "start_time": "2019-06-21T03:04:22.633771Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:37:35.830305Z",
     "start_time": "2019-06-21T04:37:35.498103Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './'\n",
    "vocab = pickle.load(open(path + 'vocab.pkl', \"rb\" ))\n",
    "inv_vocab = pickle.load(open(path + 'inv_vocab.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-21T04:37:59.071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...</td>\n",
       "      <td>[1, 148, 17, 149, 150, 112, 151, 136, 60, 79, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 742, 743, 11, 646, 307, 744, 132, 596, 745...</td>\n",
       "      <td>[1, 11, 762, 763, 764, 769, 770, 771, 272, 772...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 910, 940, 840, 941, 132, 942, 569, 943, 94...</td>\n",
       "      <td>[1, 11, 954, 947, 948, 949, 950, 72, 951, 1027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 136, 1153, 1154, 910, 1155, 1156, 1157, 11...</td>\n",
       "      <td>[1, 910, 1155, 1156, 1157, 1158, 265, 167, 422...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1413, 132, 1414, 1415, 1416, 1417, 1418, 3...</td>\n",
       "      <td>[1, 1510, 1435, 1427, 1428, 1511, 1413, 17, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...   \n",
       "1  [1, 742, 743, 11, 646, 307, 744, 132, 596, 745...   \n",
       "2  [1, 910, 940, 840, 941, 132, 942, 569, 943, 94...   \n",
       "3  [1, 136, 1153, 1154, 910, 1155, 1156, 1157, 11...   \n",
       "4  [1, 1413, 132, 1414, 1415, 1416, 1417, 1418, 3...   \n",
       "\n",
       "                                             summary  \n",
       "0  [1, 148, 17, 149, 150, 112, 151, 136, 60, 79, ...  \n",
       "1  [1, 11, 762, 763, 764, 769, 770, 771, 272, 772...  \n",
       "2  [1, 11, 954, 947, 948, 949, 950, 72, 951, 1027...  \n",
       "3  [1, 910, 1155, 1156, 1157, 1158, 265, 167, 422...  \n",
       "4  [1, 1510, 1435, 1427, 1428, 1511, 1413, 17, 15...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './'\n",
    "df = pickle.load(open(path + 'train_df.pkl', \"rb\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:01:42.896751Z",
     "start_time": "2019-06-21T04:01:42.892838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.text[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T20:59:18.608736Z",
     "start_time": "2019-06-20T20:59:18.605224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27578\n",
      "bro\n"
     ]
    }
   ],
   "source": [
    "print(vocab['bro'])\n",
    "print(inv_vocab[27578])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T22:54:59.668354Z",
     "start_time": "2019-06-20T22:54:59.658220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...</td>\n",
       "      <td>[1, 148, 17, 149, 150, 112, 151, 136, 60, 79, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 742, 743, 11, 646, 307, 744, 132, 596, 745...</td>\n",
       "      <td>[1, 11, 762, 763, 764, 769, 770, 771, 272, 772...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 910, 940, 840, 941, 132, 942, 569, 943, 94...</td>\n",
       "      <td>[1, 11, 954, 947, 948, 949, 950, 72, 951, 1027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 136, 1153, 1154, 910, 1155, 1156, 1157, 11...</td>\n",
       "      <td>[1, 910, 1155, 1156, 1157, 1158, 265, 167, 422...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1413, 132, 1414, 1415, 1416, 1417, 1418, 3...</td>\n",
       "      <td>[1, 1510, 1435, 1427, 1428, 1511, 1413, 17, 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, ...   \n",
       "1  [1, 742, 743, 11, 646, 307, 744, 132, 596, 745...   \n",
       "2  [1, 910, 940, 840, 941, 132, 942, 569, 943, 94...   \n",
       "3  [1, 136, 1153, 1154, 910, 1155, 1156, 1157, 11...   \n",
       "4  [1, 1413, 132, 1414, 1415, 1416, 1417, 1418, 3...   \n",
       "\n",
       "                                             summary  \n",
       "0  [1, 148, 17, 149, 150, 112, 151, 136, 60, 79, ...  \n",
       "1  [1, 11, 762, 763, 764, 769, 770, 771, 272, 772...  \n",
       "2  [1, 11, 954, 947, 948, 949, 950, 72, 951, 1027...  \n",
       "3  [1, 910, 1155, 1156, 1157, 1158, 265, 167, 422...  \n",
       "4  [1, 1510, 1435, 1427, 1428, 1511, 1413, 17, 15...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:22:42.287768Z",
     "start_time": "2019-06-21T04:22:42.259456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 4, 6]\n",
    "torch.Tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:15.196743Z",
     "start_time": "2019-06-21T04:30:14.980487Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (sentences, labels).\n",
    "    \n",
    "    Need custom collate_fn because merging sequences (including padding) is not \n",
    "    supported in default. Sequences are padded to the maximum length of mini-batch \n",
    "    sequences (dynamic padding).\n",
    "    \n",
    "    Args:\n",
    "        data: list of tuple (article, summary). \n",
    "            - each is list of word indices of variable length\n",
    "    Returns:\n",
    "        packed_batch: (PackedSequence), see torch.nn.utils.rnn.pack_padded_sequence\n",
    "        sencences: torch tensor of shape (batch_size, max_len).\n",
    "        labels: torch tensor of shape (batch_size, 1).\n",
    "        lengths: list; valid length for each padded sentence. \n",
    "    \"\"\"\n",
    "    # Sort a data list by sentences length (descending order).\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    articles, summaries = zip(*data)\n",
    "        \n",
    "    # Merge sentences\n",
    "    lengths1 = [len(s) for s in articles]\n",
    "    lengths2 = [len(s) for s in summaries]\n",
    "   \n",
    "    arts = torch.zeros(len(articles), max(lengths1)).long()\n",
    "    summs = torch.zeros(len(summaries), max(lengths2)).long()\n",
    "    \n",
    "    for i, a in enumerate(articles):\n",
    "        l = lengths1[i]\n",
    "        arts[i, -l:] = torch.Tensor(a[:l])\n",
    "        \n",
    "    for i, s in enumerate(summaries):\n",
    "        l = lengths2[i]\n",
    "        summs[i, :l] = torch.Tensor(s[:l])\n",
    "    \n",
    "    return arts, summs, lengths1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:15.335690Z",
     "start_time": "2019-06-21T04:30:15.331289Z"
    }
   },
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, df): #pairs, input_lang, output_lang):\n",
    "        self.df = df\n",
    "        self.article = df.text.values\n",
    "        self.summary = df.summary.values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.article)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.article[idx]\n",
    "        y = self.summary[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:15.630665Z",
     "start_time": "2019-06-21T04:30:15.628566Z"
    }
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "# df['text'].apply(lambda x: ast.literal_eval(x))\n",
    "# df['summary'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    995041.000000\n",
       " mean        587.993332\n",
       " std         764.028830\n",
       " min           2.000000\n",
       " 25%         246.000000\n",
       " 50%         467.000000\n",
       " 75%         751.000000\n",
       " max      102471.000000\n",
       " Name: text, dtype: float64, count    995041.000000\n",
       " mean         25.583946\n",
       " std          23.694143\n",
       " min           2.000000\n",
       " 25%          16.000000\n",
       " 50%          22.000000\n",
       " 75%          28.000000\n",
       " max        5678.000000\n",
       " Name: summary, dtype: float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_lens = df.text.map(lambda x: len(x))\n",
    "summary_lens = df.summary.map(lambda x: len(x))\n",
    "text_lens.describe(), summary_lens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55508, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(text_lens > 246) &(text_lens < 467) & (summary_lens < 22) & (summary_lens > 16)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:16.326757Z",
     "start_time": "2019-06-21T04:30:15.900215Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:16.331393Z",
     "start_time": "2019-06-21T04:30:16.328775Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = SummarizationDataset(train)\n",
    "valid_ds = SummarizationDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:16.517551Z",
     "start_time": "2019-06-21T04:30:16.514012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2435,\n",
       " 185,\n",
       " 38,\n",
       " 1534,\n",
       " 423,\n",
       " 1375,\n",
       " 14852,\n",
       " 15689,\n",
       " 5015,\n",
       " 230,\n",
       " 207,\n",
       " 1325,\n",
       " 106,\n",
       " 11,\n",
       " 2214,\n",
       " 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:17.008644Z",
     "start_time": "2019-06-21T04:30:16.990744Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:17.492115Z",
     "start_time": "2019-06-21T04:30:17.428387Z"
    }
   },
   "outputs": [],
   "source": [
    "arts, summs, l = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:30:18.217978Z",
     "start_time": "2019-06-21T04:30:18.214074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 395]), torch.Size([5, 21]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts.shape, summs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Seq2Seq Model\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <https://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "![](imgs/encoder-network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/msds/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word2vec_path = '~/Downloads/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_vecs, inv_vocab, D=300):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    V = len(inv_vocab)\n",
    "    W = np.random.randn(V, D)\n",
    "\n",
    "    for i in range(V):\n",
    "        if inv_vocab[i] in word_vecs:\n",
    "            W[i] = word_vecs[inv_vocab[i]]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00402832, -0.24707031,  0.09814453, ...,  0.1640625 ,\n",
       "         0.24023438,  0.6875    ],\n",
       "       [-0.19433594, -0.05932617, -0.18066406, ..., -0.12988281,\n",
       "        -0.15527344,  0.14941406],\n",
       "       [-0.15039062, -0.03063965,  0.02770996, ...,  0.11132812,\n",
       "         0.06225586,  0.04003906],\n",
       "       ...,\n",
       "       [ 0.04492188,  0.05664062,  0.09863281, ..., -0.03173828,\n",
       "         0.125     ,  0.05371094],\n",
       "       [-0.11425781,  0.12109375, -0.04418945, ...,  0.10351562,\n",
       "         0.08203125,  0.08154297],\n",
       "       [ 0.0402832 , -0.03149414, -0.15332031, ..., -0.00570679,\n",
       "        -0.06396484, -0.15625   ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "\n",
    "emb_matrix = create_embedding_matrix(word2vec, inv_vocab, embedding_dim)\n",
    "emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:34:29.831839Z",
     "start_time": "2019-06-21T04:34:29.826612Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoder is RNN\n",
    "# input size is number of words in french vocabulary\n",
    "# choose hidden size for ...\n",
    "# embedding layer, gru , and dropout\n",
    "# get output and hidden, output both of them\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embeddings(x)\n",
    "        #x = self.dropout(x)\n",
    "        pack = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, hidden = self.gru(pack)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:34:30.441996Z",
     "start_time": "2019-06-21T04:34:30.378714Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y, l = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:34:30.670602Z",
     "start_time": "2019-06-21T04:34:30.664677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,   118, 10701,  ...,   163,   209,     2],\n",
       "         [    0,     0,     0,  ...,   106,   167,     2],\n",
       "         [    0,     0,     0,  ...,   843,   763,     2],\n",
       "         [    0,     0,     0,  ...,    69,    59,     2],\n",
       "         [    0,     0,     0,  ...,  1600, 11884,     2]]),\n",
       " tensor([[    1,   494,   739,   348,   738,   601,  1052, 11593,   496,   272,\n",
       "            650,   811,   551,  2515, 13211,  1987,   288,     2,     0,     0,\n",
       "              0],\n",
       "         [    1,  3213,   132,  3933,  6489, 17268,  1534,  7997,    11,  1905,\n",
       "           1534,  3213,   201,  1276,  3393, 11239,    11, 45890, 15164,  5854,\n",
       "              2],\n",
       "         [    1,  1855,    11,    83,  2632,   211,   212, 11567,   371,   132,\n",
       "           2525, 11246,   272,   407,    11,  3108,  7772,  5341,  3369,     2,\n",
       "              0],\n",
       "         [    1,  3720,  3815,    17,   512,  3208, 10934,   652,   404,   334,\n",
       "            288,   114, 43133,   146,    58, 18873, 10244,  6931,   118,  3108,\n",
       "              2],\n",
       "         [    1,   118,   163, 13502, 22945,  1600,   215,   154,  1333, 13519,\n",
       "           5513,   194,  1572,    17,    11,  1868,     2,     0,     0,     0,\n",
       "              0]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:34:31.452711Z",
     "start_time": "2019-06-21T04:34:31.010817Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hidden_size = 300\n",
    "encoder = EncoderRNN(vocab_size, embedding_dim, hidden_size, emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:34:32.214942Z",
     "start_time": "2019-06-21T04:34:31.454466Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_outputs, enc_hidden = encoder(x.long(), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:34:36.445011Z",
     "start_time": "2019-06-21T04:34:36.436416Z"
    }
   },
   "outputs": [],
   "source": [
    "# enc_outputs.shape, enc_hidden.shape\n",
    "# what is size of encoder output and encoder hidden\n",
    "# 5 is batch size, 17 is max length\n",
    "# 5 is last state of each of 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T04:35:20.535689Z",
     "start_time": "2019-06-21T04:35:20.521276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 1.0767e-02,  1.0206e-01, -1.6842e-02,  ...,  9.9704e-03,\n",
       "         -1.9617e-02,  9.1955e-03],\n",
       "        [ 1.0299e-01, -1.3484e-02,  4.1539e-02,  ...,  3.1923e-02,\n",
       "          5.0067e-03, -9.6081e-02],\n",
       "        [ 1.0299e-01, -1.3484e-02,  4.1539e-02,  ...,  3.1923e-02,\n",
       "          5.0067e-03, -9.6081e-02],\n",
       "        ...,\n",
       "        [ 2.9758e-02, -2.9010e-03, -4.5791e-02,  ..., -1.6285e-02,\n",
       "         -1.2732e-01, -1.6537e-02],\n",
       "        [ 2.3324e-03,  5.3499e-03, -4.6229e-02,  ..., -1.3813e-02,\n",
       "         -1.1137e-01,  2.2741e-02],\n",
       "        [ 1.5945e-02,  4.2321e-02,  1.2166e-04,  ...,  1.3881e-02,\n",
       "         -6.4848e-02,  6.5394e-02]], grad_fn=<CatBackward>), batch_sizes=tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Decoder\n",
    "   -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder has embedding layer, gru, \n",
    "# output size is size of english vocabulary\n",
    "# loss function is trying to produce \n",
    "# output is taking the hidden state of decoder,\n",
    "# going through linear layer to try to produce \"the\"\n",
    "# run the decoder (GRU) word by word, because we need \"the\" to predict\n",
    "# next word, \"poor\"\n",
    "# sometimes we use the prediction or sometimes we use the actual\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=0)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(hidden[-1]) # output is a function of the hidden, what we are comparing to the y\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = vocab_size\n",
    "hidden_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SOS_token = 1\n",
    "batch_size = y.size(0)\n",
    "decoder_input = SOS_token*torch.ones(batch_size,1).long()\n",
    "decoder_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(output_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = decoder(decoder_input, enc_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 300]), torch.Size([5, 140326]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(x, y, l1, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    #two models so two optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size = y.size(0)\n",
    "    target_length = y.size(1)\n",
    "\n",
    "    enc_outputs, enc_hidden = encoder(x, l1)\n",
    "\n",
    "    loss = 0\n",
    "    dec_input = y[:,0].unsqueeze(1) # allways SOS (ec always a 1 which is index of start of sequence)\n",
    "    hidden = enc_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(1, target_length):\n",
    "        output, hidden = decoder(dec_input, hidden) # getting new hidden and output\n",
    "        # output is prediction, bunch of probabilities (kind of) for each of the words in vocab\n",
    "        yi =  y[:, di]\n",
    "        if (yi>0).sum() > 0:\n",
    "            # ignoring padding\n",
    "            # ec computing loss to ignore index 0, padding gets ignored\n",
    "            # summing so can divide over number of non-zeros that we have\n",
    "            loss += F.cross_entropy(output, yi, ignore_index = 0, reduction=\"sum\")/(yi>0).sum()\n",
    "        if use_teacher_forcing:\n",
    "            # need to decide what is next input\n",
    "            # by teacher forcing, help at the beginning to make things go faster\n",
    "            dec_input = y[:, di].unsqueeze(1)  # Teacher forcing: Feed the target as the next input\n",
    "        else:                \n",
    "            dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "    # loss depends on all the parameters. Produce gradients for all the paramters\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 10,\n",
    "          teacher_forcing_ratio=0.5):\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "#        for x, y in train_dl:\n",
    "        for x, y, l1 in train_dl:\n",
    "            x = x.long()#.cuda()\n",
    "            y = y.long()#.cuda()\n",
    "            loss = train_batch(x, y, l1, encoder, decoder, enc_optimizer, dec_optimizer,\n",
    "                               teacher_forcing_ratio)\n",
    "            total_loss = loss*x.size(0)\n",
    "            total += x.size(0)\n",
    "        if i%10 == 0:\n",
    "            print(\"train loss %.3f\" % (total_loss / total))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size\n",
    "output_size = vocab_size\n",
    "hidden_size = 300\n",
    "encoder = EncoderRNN(vocab_size, embedding_dim, hidden_size, emb_matrix) # .cuda()\n",
    "decoder = DecoderRNN(output_size, hidden_size) # .cuda()\n",
    "# same thing just twice\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=0.01)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, collate_fn=collate_fn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.128\n",
      "train loss 1.058\n",
      "train loss 0.338\n",
      "train loss 0.299\n"
     ]
    }
   ],
   "source": [
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=0.001) \n",
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.818\n",
      "train loss 0.836\n",
      "train loss 0.806\n",
      "train loss 0.738\n",
      "train loss 0.808\n",
      "train loss 0.657\n",
      "train loss 0.688\n",
      "train loss 0.580\n",
      "train loss 0.561\n",
      "train loss 0.540\n",
      "train loss 0.636\n",
      "train loss 0.624\n",
      "train loss 0.588\n",
      "train loss 0.484\n",
      "train loss 0.490\n",
      "train loss 0.501\n",
      "train loss 0.489\n",
      "train loss 0.463\n",
      "train loss 0.445\n",
      "train loss 0.478\n",
      "train loss 0.389\n",
      "train loss 0.412\n",
      "train loss 0.348\n",
      "train loss 0.462\n",
      "train loss 0.416\n",
      "train loss 0.544\n",
      "train loss 0.332\n",
      "train loss 0.380\n",
      "train loss 0.500\n",
      "train loss 0.417\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 300, teacher_forcing_ratio=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.335\n",
      "train loss 0.292\n",
      "train loss 0.306\n",
      "train loss 0.311\n",
      "train loss 0.280\n",
      "train loss 0.345\n",
      "train loss 0.372\n",
      "train loss 0.290\n",
      "train loss 0.262\n",
      "train loss 0.355\n",
      "train loss 0.258\n",
      "train loss 0.352\n",
      "train loss 0.252\n",
      "train loss 0.444\n",
      "train loss 0.236\n",
      "train loss 0.238\n",
      "train loss 0.257\n",
      "train loss 0.266\n",
      "train loss 0.240\n",
      "train loss 0.237\n",
      "train loss 0.248\n",
      "train loss 0.321\n",
      "train loss 0.247\n",
      "train loss 0.169\n",
      "train loss 0.208\n",
      "train loss 0.207\n",
      "train loss 0.206\n",
      "train loss 0.244\n",
      "train loss 0.198\n",
      "train loss 0.172\n"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, enc_optimizer, dec_optimizer, epochs = 300, teacher_forcing_ratio=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `model.eval()` will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.\n",
    "* `torch.no_grad()` impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ec torch.no_grad() makes faster and more efficient\n",
    "def decoding(x, y, encoder, decoder, max_length=MAX_LENGTH+2):\n",
    "    decoder = decoder.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():   \n",
    "        batch_size = x.size(0)\n",
    "        enc_outputs, hidden = encoder(x)\n",
    "        dec_input = SOS_token*torch.ones(batch_size, 1).long().cuda()  # SOS\n",
    "        decoded_words = []\n",
    "        # ec decide in advance max length. how big are we going to allow the output to be?\n",
    "        for di in range(1, max_length):\n",
    "            output, hidden = decoder(dec_input, hidden)\n",
    "            pred = output.argmax(dim=1) # ec this is hard prediction (index of right word)\n",
    "            # ec bc we want to keep the prediction around\n",
    "            decoded_words.append(pred.cpu().numpy())\n",
    "            dec_input = output.argmax(dim=1).unsqueeze(1).detach()\n",
    "            yi =  y[:, di]\n",
    "            # without if you will get a None or NA(?) due to divide by zero\n",
    "            if (yi>0).sum() > 0:\n",
    "                # ignoring padding\n",
    "                loss += F.cross_entropy(\n",
    "                    output, yi, ignore_index = 0, reduction=\"sum\")/(yi>0).sum()\n",
    "        return loss.item()/batch_size, np.transpose(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14845184326171876"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=300\n",
    "valid_dl_2 = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(valid_dl_2)) \n",
    "x = x.long().cuda()\n",
    "y = y.long().cuda()\n",
    "\n",
    "loss, _ = decoding(x, y, encoder, decoder)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl_2 = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_dl_2)) \n",
    "x = x.long().cuda()\n",
    "y = y.long().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(x, y, encoder, decoder):\n",
    "    _, decoded_words = decoding(x, y, encoder, decoder)\n",
    "    for i in range(x.shape[0]):\n",
    "        xi = x[i].cpu().numpy()\n",
    "        yi = y[i].cpu().numpy()\n",
    "        y_hat = decoded_words[i]\n",
    "        x_sent = ' '.join([input_lang.index2word[t] for t in xi if t > 3])\n",
    "        y_sent = ' '.join([output_lang.index2word[t] for t in yi if t > 3])\n",
    "        y_hat_sent = ' '.join([output_lang.index2word[t] for t in y_hat if t > 3])\n",
    "        print('>', x_sent)\n",
    "        print('=', y_sent)\n",
    "        print('<', y_hat_sent)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis quelqu un de bien .\n",
      "= i m a nice guy .\n",
      "< i m a nice guy .\n",
      "\n",
      "> vous etes rusee .\n",
      "= you re crafty .\n",
      "< you re crafty .\n",
      "\n",
      "> je suis un peu desoriente .\n",
      "= i m a little confused .\n",
      "< i m a little confused .\n",
      "\n",
      "> actuellement je me trouve a l aeroport de narita .\n",
      "= i m at narita airport right now .\n",
      "< i m at narita airport right now .\n",
      "\n",
      "> je suis juste ici .\n",
      "= i m right here .\n",
      "< i m just here .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(x, y, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "valid_dl_2 = DataLoader(valid_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x, y = next(iter(valid_dl_2)) \n",
    "x = x.long().cuda()\n",
    "y = y.long().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je ne suis pas cette sorte de fille .\n",
      "= i m not that kind of girl .\n",
      "< i m not in a of girl .\n",
      "\n",
      "> nous sommes en securite ici .\n",
      "= we re safe here .\n",
      "< we re here here . couple .\n",
      "\n",
      "> ils le font correctement .\n",
      "= they re doing it right .\n",
      "< they re doing it right .\n",
      "\n",
      "> j ai raison .\n",
      "= i m right .\n",
      "< i m correct .\n",
      "\n",
      "> vous etes tres avises .\n",
      "= you re very wise .\n",
      "< you re very wise .\n",
      "\n",
      "> nous nous marions .\n",
      "= we re getting married .\n",
      "< we re undressing .\n",
      "\n",
      "> j ai une mauvaise impression .\n",
      "= i m getting a bad feeling .\n",
      "< i m a of . .\n",
      "\n",
      "> ce n est pas le genre de type a abandonner facilement .\n",
      "= he is not the sort of guy who gives in easily .\n",
      "< he s not very well off at at s . .\n",
      "\n",
      "> c est un homme cruel .\n",
      "= he is a cruel person .\n",
      "< he s a man of .\n",
      "\n",
      "> ils ne sont pas plus semblables qu une vache a un canari .\n",
      "= they are no more alike than a cow and a canary .\n",
      "< they are as a rock rock band in . .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(x, y, encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "-  Replace the embeddings with pre-trained word embeddings. Here are word embeddings for various languages.\n",
    "\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "The original notebook was written by Sean Robertson <https://github.com/spro/practical-pytorch>_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
